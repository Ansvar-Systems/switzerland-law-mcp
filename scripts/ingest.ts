#!/usr/bin/env tsx
/**
 * Switzerland Law MCP — Ingestion Pipeline
 *
 * Fetches Swiss federal legislation from Fedlex (fedlex.admin.ch).
 * Fedlex provides Open Government Data for all Swiss federal legislation.
 *
 * Strategy:
 * 1. Read data/census.json (generated by scripts/census.ts)
 * 2. Batch-resolve HTML URLs via SPARQL POST (500 acts per query)
 * 3. Fetch HTML with rate limiting (500ms between requests)
 * 4. Parse articles from the structured HTML
 * 5. Write seed JSON files for the database builder
 *
 * Resume support: Acts with existing seed files in data/seed/ are skipped.
 *
 * Usage:
 *   npm run ingest                    # Full ingestion
 *   npm run ingest -- --limit 50      # Test with 50 acts
 *   npm run ingest -- --skip-fetch    # Reuse cached HTML pages
 *
 * Data is sourced under Open Government Data principles.
 */

import * as fs from 'fs';
import * as path from 'path';
import { fileURLToPath } from 'url';
import { fetchWithRateLimit } from './lib/fetcher.js';
import { parseFedlexHtml, censusEntryToAct, type ActIndexEntry } from './lib/parser.js';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

const SOURCE_DIR = path.resolve(__dirname, '../data/source');
const SEED_DIR = path.resolve(__dirname, '../data/seed');
const CENSUS_PATH = path.resolve(__dirname, '../data/census.json');
const SPARQL_ENDPOINT = 'https://fedlex.data.admin.ch/sparqlendpoint';
const USER_AGENT = 'Switzerland-Law-MCP/1.0 (https://github.com/Ansvar-Systems/switzerland-law-mcp; hello@ansvar.ai)';
const BATCH_SIZE = 500;

function parseArgs(): { limit: number | null; skipFetch: boolean; offset: number } {
  const args = process.argv.slice(2);
  let limit: number | null = null;
  let skipFetch = false;
  let offset = 0;

  for (let i = 0; i < args.length; i++) {
    if (args[i] === '--limit' && args[i + 1]) {
      limit = parseInt(args[i + 1], 10);
      i++;
    } else if (args[i] === '--offset' && args[i + 1]) {
      offset = parseInt(args[i + 1], 10);
      i++;
    } else if (args[i] === '--skip-fetch') {
      skipFetch = true;
    }
  }

  return { limit, skipFetch, offset };
}

interface CensusFile {
  total_acts: number;
  acts: Array<{
    id: string;
    sr_number: string;
    title: string;
    title_en: string;
    eli_uri: string;
    classification: string;
  }>;
}

interface ResolvedUrl {
  url: string;
  language: 'en' | 'de';
}

interface IngestionStats {
  total: number;
  skipped: number;
  success: number;
  noHtml: number;
  failed: number;
  totalProvisions: number;
  totalDefinitions: number;
}

function loadCensus(): Array<{ act: ActIndexEntry; eliUri: string }> {
  if (!fs.existsSync(CENSUS_PATH)) {
    console.error(`ERROR: Census file not found at ${CENSUS_PATH}`);
    console.error('Run "npx tsx scripts/census.ts" first.');
    process.exit(1);
  }

  const raw = JSON.parse(fs.readFileSync(CENSUS_PATH, 'utf-8')) as CensusFile;
  console.log(`  Census: ${raw.total_acts} acts`);

  return raw.acts
    .filter(a => a.classification === 'ingestable')
    .map(a => ({
      act: censusEntryToAct(a),
      eliUri: a.eli_uri,
    }));
}

/**
 * Batch-resolve HTML URLs for a set of acts via SPARQL POST.
 * Returns a Map from work URI to resolved HTML URL + language.
 */
async function batchResolveHtmlUrls(
  workUris: string[],
): Promise<Map<string, ResolvedUrl>> {
  const values = workUris.map(u => `<${u}>`).join(' ');

  const query = `
    PREFIX jolux: <http://data.legilux.public.lu/resource/ontology/jolux#>
    SELECT ?work (SAMPLE(?urlEn) AS ?htmlEn) (SAMPLE(?urlDe) AS ?htmlDe) WHERE {
      VALUES ?work { ${values} }
      OPTIONAL {
        ?conEn jolux:isMemberOf ?work .
        ?conEn jolux:isRealizedBy ?exprEn .
        ?exprEn jolux:language <http://publications.europa.eu/resource/authority/language/ENG> .
        ?exprEn jolux:isEmbodiedBy ?manifEn .
        ?manifEn jolux:userFormat <https://fedlex.data.admin.ch/vocabulary/user-format/html> .
        ?manifEn jolux:isExemplifiedBy ?urlEn .
      }
      OPTIONAL {
        ?conDe jolux:isMemberOf ?work .
        ?conDe jolux:isRealizedBy ?exprDe .
        ?exprDe jolux:language <http://publications.europa.eu/resource/authority/language/DEU> .
        ?exprDe jolux:isEmbodiedBy ?manifDe .
        ?manifDe jolux:userFormat <https://fedlex.data.admin.ch/vocabulary/user-format/html> .
        ?manifDe jolux:isExemplifiedBy ?urlDe .
      }
    }
    GROUP BY ?work
  `;

  const body = `query=${encodeURIComponent(query)}`;

  for (let attempt = 0; attempt < 5; attempt++) {
    try {
      const response = await fetch(SPARQL_ENDPOINT, {
        method: 'POST',
        headers: {
          'User-Agent': USER_AGENT,
          'Accept': 'application/sparql-results+json',
          'Content-Type': 'application/x-www-form-urlencoded',
        },
        body,
      });

      if (response.status === 429 || response.status >= 500) {
        const backoff = Math.pow(2, attempt + 1) * 2000;
        console.log(`    SPARQL HTTP ${response.status}, retrying in ${backoff / 1000}s...`);
        await new Promise(resolve => setTimeout(resolve, backoff));
        continue;
      }

      if (response.status !== 200) {
        throw new Error(`SPARQL POST failed: HTTP ${response.status}`);
      }

      const text = await response.text();
      const json = JSON.parse(text);
      const bindings = json.results?.bindings ?? [];

      const result = new Map<string, ResolvedUrl>();
      for (const b of bindings) {
        const work = b.work.value;
        // Prefer English, fall back to German
        if (b.htmlEn?.value) {
          result.set(work, { url: b.htmlEn.value, language: 'en' });
        } else if (b.htmlDe?.value) {
          result.set(work, { url: b.htmlDe.value, language: 'de' });
        }
      }

      return result;
    } catch (err) {
      if (attempt < 4) {
        const backoff = Math.pow(2, attempt + 1) * 1000;
        console.log(`    Network error, retrying in ${backoff / 1000}s...`);
        await new Promise(resolve => setTimeout(resolve, backoff));
        continue;
      }
      throw err;
    }
  }

  throw new Error('SPARQL batch resolve failed after 5 retries');
}

/**
 * Resolve HTML URLs for all acts in batches.
 */
async function resolveAllHtmlUrls(
  entries: Array<{ act: ActIndexEntry; eliUri: string }>,
): Promise<Map<string, ResolvedUrl>> {
  console.log(`\n  Phase 1: Batch-resolving HTML URLs (${BATCH_SIZE} per query)...\n`);

  const allResolved = new Map<string, ResolvedUrl>();
  const totalBatches = Math.ceil(entries.length / BATCH_SIZE);

  for (let i = 0; i < entries.length; i += BATCH_SIZE) {
    const batch = entries.slice(i, i + BATCH_SIZE);
    const batchNum = Math.floor(i / BATCH_SIZE) + 1;
    process.stdout.write(`    Batch ${batchNum}/${totalBatches} (${batch.length} acts)...`);

    const uris = batch.map(e => e.eliUri);
    const resolved = await batchResolveHtmlUrls(uris);

    for (const [k, v] of resolved) {
      allResolved.set(k, v);
    }

    console.log(` ${resolved.size} with HTML`);

    // Small delay between batches
    if (i + BATCH_SIZE < entries.length) {
      await new Promise(resolve => setTimeout(resolve, 500));
    }
  }

  console.log(`\n  Resolved: ${allResolved.size} of ${entries.length} acts have HTML`);
  return allResolved;
}

async function main(): Promise<void> {
  const { limit, skipFetch, offset } = parseArgs();

  console.log('Switzerland Law MCP — Ingestion Pipeline');
  console.log('=========================================\n');
  console.log(`  Source: Fedlex (fedlex.admin.ch)`);
  console.log(`  License: Open Government Data`);
  console.log(`  Strategy: Batch SPARQL resolve -> HTML fetch -> parse`);

  if (limit) console.log(`  --limit ${limit}`);
  if (offset) console.log(`  --offset ${offset}`);
  if (skipFetch) console.log(`  --skip-fetch`);

  const allEntries = loadCensus();

  let entries = allEntries;
  if (offset > 0) entries = entries.slice(offset);
  if (limit) entries = entries.slice(0, limit);

  console.log(`  Processing: ${entries.length} of ${allEntries.length} acts`);

  fs.mkdirSync(SOURCE_DIR, { recursive: true });
  fs.mkdirSync(SEED_DIR, { recursive: true });

  // Identify which acts need processing (no existing seed)
  const needsProcessing: Array<{ act: ActIndexEntry; eliUri: string }> = [];
  let skipCount = 0;
  let skipProvisions = 0;

  for (const entry of entries) {
    const seedFile = path.join(SEED_DIR, `${entry.act.id}.json`);
    if (fs.existsSync(seedFile)) {
      skipCount++;
      try {
        const existing = JSON.parse(fs.readFileSync(seedFile, 'utf-8'));
        skipProvisions += existing.provisions?.length ?? 0;
      } catch { /* ignore */ }
    } else {
      needsProcessing.push(entry);
    }
  }

  if (skipCount > 0) {
    console.log(`  Skipping: ${skipCount} acts with existing seeds (${skipProvisions} provisions)`);
  }
  console.log(`  To fetch: ${needsProcessing.length} acts`);

  if (needsProcessing.length === 0) {
    console.log('\n  All acts already have seed files. Nothing to do.');
    printFinalStats(entries);
    return;
  }

  // Phase 1: Batch-resolve HTML URLs
  const resolvedUrls = await resolveAllHtmlUrls(needsProcessing);

  // Phase 2: Fetch and parse HTML
  console.log(`\n  Phase 2: Fetching and parsing HTML...\n`);

  const stats: IngestionStats = {
    total: entries.length,
    skipped: skipCount,
    success: 0,
    noHtml: 0,
    failed: 0,
    totalProvisions: skipProvisions,
    totalDefinitions: 0,
  };

  const failedActs: Array<{ id: string; sr: string; error: string }> = [];

  for (let i = 0; i < needsProcessing.length; i++) {
    const { act, eliUri } = needsProcessing[i];
    const sourceFile = path.join(SOURCE_DIR, `${act.id}.html`);
    const seedFile = path.join(SEED_DIR, `${act.id}.json`);

    const resolved = resolvedUrls.get(eliUri);
    if (!resolved) {
      stats.noHtml++;
      // Log only every 100th missing HTML to avoid spam
      if (stats.noHtml <= 10 || stats.noHtml % 100 === 0) {
        console.log(`  [${i + 1}/${needsProcessing.length}] ${act.id} — no HTML`);
      }
      continue;
    }

    try {
      let html: string;

      if (fs.existsSync(sourceFile) && skipFetch) {
        html = fs.readFileSync(sourceFile, 'utf-8');
      } else {
        const result = await fetchWithRateLimit(resolved.url);
        if (result.status !== 200) {
          stats.failed++;
          failedActs.push({ id: act.id, sr: act.srNumber, error: `HTTP ${result.status}` });
          continue;
        }

        html = result.body;

        // Validate content
        if (!html.includes('lawcontent') && !html.includes('<article')) {
          stats.noHtml++;
          continue;
        }

        fs.writeFileSync(sourceFile, html);
      }

      const parsed = parseFedlexHtml(html, act);
      fs.writeFileSync(seedFile, JSON.stringify(parsed, null, 2));

      stats.success++;
      stats.totalProvisions += parsed.provisions.length;
      stats.totalDefinitions += parsed.definitions.length;

      // Progress logging every 50 successes
      if (stats.success % 50 === 0) {
        console.log(`  --- ${stats.success} ingested, ${stats.noHtml} no-HTML, ${stats.failed} failed, ${stats.totalProvisions} provisions ---`);
      }
    } catch (error) {
      const msg = error instanceof Error ? error.message : String(error);
      stats.failed++;
      failedActs.push({ id: act.id, sr: act.srNumber, error: msg });

      if (stats.failed <= 10 || stats.failed % 50 === 0) {
        console.log(`  ERROR ${act.id}: ${msg}`);
      }
    }
  }

  // Final report
  console.log(`\n${'='.repeat(72)}`);
  console.log('INGESTION REPORT');
  console.log('='.repeat(72));
  console.log(`  Total census acts:  ${stats.total}`);
  console.log(`  Skipped (cached):   ${stats.skipped}`);
  console.log(`  Newly ingested:     ${stats.success}`);
  console.log(`  No HTML available:  ${stats.noHtml}`);
  console.log(`  Failed:             ${stats.failed}`);
  console.log(`  Total provisions:   ${stats.totalProvisions}`);
  console.log(`  Total definitions:  ${stats.totalDefinitions}`);

  if (failedActs.length > 0 && failedActs.length <= 50) {
    console.log(`\n  Failed acts:`);
    for (const f of failedActs) {
      console.log(`    ${f.id} (SR ${f.sr}) — ${f.error}`);
    }
  }

  console.log('='.repeat(72));

  // Also print final stats from all seed files
  printFinalStats(entries);
}

function printFinalStats(entries: Array<{ act: ActIndexEntry; eliUri: string }>): void {
  // Count all seed files and provisions
  const seedFiles = fs.readdirSync(SEED_DIR).filter(f => f.endsWith('.json'));
  let totalProvisions = 0;
  let totalDefinitions = 0;

  for (const f of seedFiles) {
    try {
      const seed = JSON.parse(fs.readFileSync(path.join(SEED_DIR, f), 'utf-8'));
      totalProvisions += seed.provisions?.length ?? 0;
      totalDefinitions += seed.definitions?.length ?? 0;
    } catch { /* ignore */ }
  }

  console.log(`\n  FINAL STATE:`);
  console.log(`    Seed files: ${seedFiles.length}`);
  console.log(`    Total provisions: ${totalProvisions}`);
  console.log(`    Total definitions: ${totalDefinitions}`);
}

main().catch(error => {
  console.error('Fatal error:', error);
  process.exit(1);
});
